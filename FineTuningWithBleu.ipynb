{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install transformers datasets nltk tensorboard py7zr accelerate evaluate --upgrade","metadata":{"id":"s53pqSYe9txe","execution":{"iopub.status.busy":"2023-06-13T06:43:13.350075Z","iopub.execute_input":"2023-06-13T06:43:13.350777Z","iopub.status.idle":"2023-06-13T06:43:42.240187Z","shell.execute_reply.started":"2023-06-13T06:43:13.350747Z","shell.execute_reply":"2023-06-13T06:43:42.238913Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%capture\n!sudo apt-get install git-lfs --yes","metadata":{"id":"_7iFA66Q95dY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_id=\"google/flan-t5-base\"\n\n# load model from the hub\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n\n# Load tokenizer of FLAN-t5-base\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"id":"badcwQ6g-ccH","colab":{"base_uri":"https://localhost:8080/","height":241,"referenced_widgets":["ed11e23fb29448bfb2c09a167ec9c39c","b9149bd5031f4fe98ebfde5de65da418","6fa04ceb19a34c3c984f86bed13821d0","0c21a875a99c4f2d913c899de73f981e","d99ce8e197a14f6580326d919d29e5ab","efe14f86d35e41b5a37db4914d76ee74","6b6fa141d27b40328287dbd1722c6fee","11d54533c1e448458bafb26222ed1825","947efdde645b4357aacec4176234ad5f","f5ff19a173274192b82804e13416888a","061cf7fabdbb4d0f9fbcb0cb7fc617aa","737ef8f6de144dd6a8eed02798c833dc","0313707da1484d99bff12faabd038468","e573b20baa5548cab3e2a6fac35b5c7a","e1f60174f5694ecdb40c51ab13a47c74","9d09e02c0f234cc5b127d99134c336e2","39946f592ad142648f44609163b8eb3e","6d6a8e179179413eb74881ab579b5d36","3a316d8470e34ea080ef4ef4a0eb7be5","387c314807ae4a68a420f2665e7a7efe","f99fdb6b34f3432fa49db4a60b684760","f968924499214648b917eb39daa9aff4","67dd002f586a4e718a9009ddb80133c2","d14cd62f36ae4161bb9a834bb49f9a10","47e673be425a43d6849d1d4fa72c06b7","be19fd5a768c4a52a0bc58da285ac9c7","c7de77e747344d7d972304d85f5ed638","be690c46b15647779ea613193e1edfd1","54aeff9e706d4734858c8c84befd348e","41f03dd53b5247939062c5e911c10896","8b4a1152638a45a99d262aaeac65188c","d073d713a42242eb93faa18c90a79030","d6f96f7b2c0b4e80bc13cb928c938250","895572caf7704e5c9f66e0bcd9208afa","7023020ee2fd488fb4db01e416dc2d8b","5eda88d3ac3a4c31bb9c20ea7097a4bf","fade252feeeb409aa5036d97763dace7","72d7f7bb8e964352b4c746c83c22ba76","9917522a030240c583759b20ec01ffa4","91a78f71041141a38d1673979bc7b2f9","fc737660d1ca4c70b8d61351fe207f3d","a256f5f11d814aaba9855a8674ec6c5c","5ae552bae22846e580525f0b72de52e1","aa3d288eb3014909b6ea8bfe6b106588","fa550f1a25d14cb9ad76a090019c2041","ae1ef2dd36914f5080b7335ae9b16d67","5498000ab0f448a29da5357106165c7f","00934c11c4054803908bbe7850efdb7d","72f1e0ea893b4677a3ee4947766deb27","cc70e101948547c4bbe2146f954d54d9","c59e46aa8459404da6fd69090a088be3","d3eef2fa54904daa88becf7ebbb6e20b","c94fa588508040398b5153631f7f86a9","5f2a70d5132545fe8834b29bf5a80ca9","e371151c2a8448849dcd8e78724b9a80","d0cb78bfa12e4007aaaf8bbe4e7ad760","acd3cc4cf75e4ad8890b4e2ad290e8cd","5922f6259a854f7493450d0b078ce392","000a3d3b3c594502bbcfa99a31bd6944","1d402e3f511d49719d397d0e04ca3220","73b46deacae9449f8522007f483b3a41","8a15025d8b2c4673b3b8df4e48094ac4","621b3c4394fb459db17754b19cb9b291","a539a753f8c04834863bb9d6f7469ae3","0481da26051b4569b8f3ab84dcbadc69","dc1982616bdb41d288699b59fc0b0856","3f601f0c1cd34d65a46523c009d02e3d","8817619b3cdc47e0a184584e3dc4812c","6f4b8095e59842369cb6f94fe541706b","41e14752b5544df4833df2343b1c5b3e","8c66e68478584c6f9549191b9562e617","3ca355f8b34e4192b0359e342bb54255","44dc96820d344eacbccb553374a8f70c","5861475100f2413c89eb4bbcaca3fc1a","34db791cccf34ab6a0a28e94eb249586","a7cefbacc33f4c35a626b3765013be34","9e27d1c5dfaf45e9846406378d6e209e"]},"executionInfo":{"status":"ok","timestamp":1686416321156,"user_tz":-330,"elapsed":43133,"user":{"displayName":"","userId":""}},"outputId":"8453b5dc-e691-4065-f855-5fde1b89d4ee","execution":{"iopub.status.busy":"2023-06-13T06:43:42.242658Z","iopub.execute_input":"2023-06-13T06:43:42.243014Z","iopub.status.idle":"2023-06-13T06:44:15.288633Z","shell.execute_reply.started":"2023-06-13T06:43:42.242977Z","shell.execute_reply":"2023-06-13T06:44:15.287636Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"269bd410d6b240179499e9ccb6b213df"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1cdd2fa45ba4c09b49b92740efdf824"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dc2ffeca0aa4b22adc4fa10eb645ba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9c674654f8c4b2382cf20149b4a4325"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4be77f984b943e9afc1742fb644ec2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3e7bb707bde41569ba1b3db4c6ce6d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73a1cd2788af4493a8d8d1e79c863726"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"Gm2JKgq2syEc","execution":{"iopub.status.busy":"2023-06-13T06:44:15.290277Z","iopub.execute_input":"2023-06-13T06:44:15.290657Z","iopub.status.idle":"2023-06-13T06:44:16.145798Z","shell.execute_reply.started":"2023-06-13T06:44:15.290625Z","shell.execute_reply":"2023-06-13T06:44:16.144737Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"json\", data_files=\"//kaggle/input/combined-data/data.json\")","metadata":{"id":"c3oUPk5JDPEp","colab":{"base_uri":"https://localhost:8080/","height":169,"referenced_widgets":["22b9c0b7da334ad18a8dab5a2c8cc56e","a83ae6f9ee8047c5a21046994ccd325a","5f8fba397d79495b9b6d5875ea9228b2","bb2d266371b94df998db76d0b759aa02","94c4491d7d6542dca2da7603565186d8","68479ddd550944b1949aa2a2587a8c84","295c74ab34c24815a3b3cda21c18bfd0","5bfcf3ba5a5a4a438fb1c7e7c51871b1","3d46fe4a464d4e7d89714c44d85e4ed7","84401dd0555c41c8bf697089d969f1b1","7e7b53c0e18b4be8991c88e0b8ec09db","81ce69ed917f4644937c8c90d890b48d","c14f3d6c01ec44cdb85250c5c9e5e4af","7c56f7d5f7b244f1bcde7e561eedb2af","12f6b7b2250c497eb23eb4bd5a8f7cd9","8b03198434f249eebf9ca1fb5d284725","afd934de4c25483a88e6bb5bbaacf840","6ef4d236e63642248d8cc6e08c02dddd","bda828e297e442c493e0b542f08ddcbd","31746e2b232742fa8fe87876ed211ac5","66db1fa745c4458ab1ac36f0fd348182","945705c574684f178e247ec0a4243a85","172ed8e569b740fb9278cefed54c8646","4467cdb8a0d9485283c490c9d67d6dbc","c9acbad105944f07840cc9cfb52f8a73","cfc9ee6b7d9446e8839f2bd5e3c75093","3860f8e80b7040c88952d4d4fc9666cd","ef6a9f78218e497daf512d16c26c4806","1055e4993fc34c7f966fb720cb87e0be","296731d0d37c4369b8d7257d37942836","26694508bc1c45b1b653a0b78e9bfc92","c7a860797ff544748bc29232e9209a88","42eee6a014854146ae9d7f01e080513e","2aea1cf869354376be55049072949cb2","be8efbb5368c45c7ad8e42b7866420f6","9e1dfee4d36a4986a82bf58a227f1458","195243af0ccd4e32a2eeed72dc3fc290","82bfb540ccd34d2ebc99e80a229c7a4f","f0c71ddebec04c90bf0afd13c6b4bf20","636681f93f574e938a062509639cd01a","9ceca4088db14b2eb49ba88a80fa3cd8","35cace3df5db444089630ffb5bf475f8","fe7bb018f30d4e71ae972c7c50f0db08","c60dde73f0c84deeb8da879b09eb25cf"]},"executionInfo":{"status":"ok","timestamp":1686416324467,"user_tz":-330,"elapsed":2445,"user":{"displayName":"","userId":""}},"outputId":"40f1e457-a737-4388-e1df-cc203d8a4f7c","execution":{"iopub.status.busy":"2023-06-13T06:44:16.148727Z","iopub.execute_input":"2023-06-13T06:44:16.149478Z","iopub.status.idle":"2023-06-13T06:44:17.338950Z","shell.execute_reply.started":"2023-06-13T06:44:16.149446Z","shell.execute_reply":"2023-06-13T06:44:17.337991Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-7f10e0875a17ad20/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f4a827fd9374279b960c4a4175bdd44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42824bc7e66e43abaa65a55960f98e42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-7f10e0875a17ad20/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ea14a502ae24abb98af3454c42ccee6"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"id":"JqBxblpfDu_X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686416324467,"user_tz":-330,"elapsed":6,"user":{"displayName":"","userId":""}},"outputId":"a0510b05-2b1f-420a-8f45-459428ff7512","execution":{"iopub.status.busy":"2023-06-13T06:44:17.340578Z","iopub.execute_input":"2023-06-13T06:44:17.341228Z","iopub.status.idle":"2023-06-13T06:44:17.349332Z","shell.execute_reply.started":"2023-06-13T06:44:17.341193Z","shell.execute_reply":"2023-06-13T06:44:17.348239Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['response', 'data'],\n        num_rows: 9207\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# train_indices, test_indices = train_test_split(range(len(dataset['train'])), test_size=0.1, random_state=42)\n\n# train_ds = dataset['train'].select(train_indices).map(lambda example: {k: example[k] for k in desired_columns})\n# test_ds = dataset['train'].select(test_indices).map(lambda example: {k: example[k] for k in desired_columns})\n\n# dataset['train'] = train_ds\n# dataset['test'] = test_ds","metadata":{"id":"sa4eEgsRDMhv","execution":{"iopub.status.busy":"2023-06-13T06:44:17.351196Z","iopub.execute_input":"2023-06-13T06:44:17.352138Z","iopub.status.idle":"2023-06-13T06:44:17.359210Z","shell.execute_reply.started":"2023-06-13T06:44:17.352105Z","shell.execute_reply":"2023-06-13T06:44:17.358028Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# from datasets import concatenate_datasets\n\n# The maximum total input sequence length after tokenization. \n# Sequences longer than this will be truncated, sequences shorter will be padded.\ntokenized_inputs = dataset[\"train\"].map(lambda x: tokenizer(x[\"data\"], truncation=True), batched=True, remove_columns=[\"data\", \"response\"])\nmax_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\nprint(f\"Max source length: {max_source_length}\")\n\n# The maximum total sequence length for target text after tokenization. \n# Sequences longer than this will be truncated, sequences shorter will be padded.\"\ntokenized_targets = dataset[\"train\"].map(lambda x: tokenizer(x[\"response\"], truncation=True), batched=True, remove_columns=[\"data\", \"response\"])\nmax_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\nprint(f\"Max target length: {max_target_length}\")","metadata":{"id":"v1LVQgB2-eX-","colab":{"base_uri":"https://localhost:8080/","height":53,"referenced_widgets":["45ecffc79b094635985a403b3f27ba08","3c4f567e6e3b4ebfbb116f5e2d96359a","4c8eb43798c642f0aa3b31e20263062c","757698f6eecc4e469ef2ebd9f2d14fe9","433d64c5d87b4b049ad80c1bc63536b7","ee24b10b0bb64cc8825d8eaa757e604f","357b613d78c04901abc74b2b87df6952","ba2d819c16b8485d86397f206fc573ca","66ff24ea29a04396b68b5d6cb20e347a","02bfcc80470e4d6dad2e5c1d74de0b08","ed535f992608401bae7537049433bd54","1d896c8cecbc498892368b468b9a4a90","6fa125455d6640a9be5c507f3cdccf00","af8747cada4e4d3184f3efb8801c3aad","ca8a03bd4232483cbc321a5c9117c6da","e2e0cf9125b041ce9b8d6bebff00ee03","052324bfe45c466583c13427b80c066c","eea4dae5cd094fb9aaffcd30c33886de","29cced952a414f3c86547a6f22a27ba4","50c93ad3f7104cc7bb339d0e6bc88439","3eb04f62696a475491b9a119e7063b7e","008e1ae0399f453a9859320e2cb1aaae"]},"executionInfo":{"status":"ok","timestamp":1686416329943,"user_tz":-330,"elapsed":5480,"user":{"displayName":"","userId":""}},"outputId":"e568e11d-1526-4ee5-d774-e49c2b975ca2","execution":{"iopub.status.busy":"2023-06-13T06:44:17.362346Z","iopub.execute_input":"2023-06-13T06:44:17.363067Z","iopub.status.idle":"2023-06-13T06:44:28.081755Z","shell.execute_reply.started":"2023-06-13T06:44:17.363010Z","shell.execute_reply":"2023-06-13T06:44:28.080062Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9207 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Max source length: 512\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9207 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Max target length: 512\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"id":"YWTIijYBtvFG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686416329944,"user_tz":-330,"elapsed":22,"user":{"displayName":"","userId":""}},"outputId":"99590650-0b42-4ca6-a03a-6b80da1a894c","execution":{"iopub.status.busy":"2023-06-13T06:44:28.083303Z","iopub.execute_input":"2023-06-13T06:44:28.084058Z","iopub.status.idle":"2023-06-13T06:44:28.090885Z","shell.execute_reply.started":"2023-06-13T06:44:28.084006Z","shell.execute_reply":"2023-06-13T06:44:28.089981Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'response': 'France',\n 'data': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries. In what country is Normandy located?'}"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(sample,padding=\"max_length\"):\n    # # add prefix to the input for t5\n    inputs = sample[\"data\"]\n\n    # # tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n\n    # Tokenize targets with the `text_target` keyword argument\n    labels = tokenizer(text_target=sample[\"response\"], max_length=max_target_length, padding=padding, truncation=True)\n\n    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n    # padding in the loss.\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"data\", \"response\"])\nprint(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")","metadata":{"id":"55MVJ0f--gRj","colab":{"base_uri":"https://localhost:8080/","height":35,"referenced_widgets":["d74ef3db54034334a17e35b01bdfd9cf","a67d2fa0d970418ab596197141538907","7fff7b2d325a49399aeb1d63739cc4fa","6f0e2215c06047b9b01169b71619e9a2","a576b990763c44ee8f081da0a4e26a90","00e0ea63156c49e7b4e49b7fc72f1c23","d814c4e12a554312bd8c22810e0e9869","293b6f8e77a84ea180d6cf00adfd7496","057f31c308db4e2080ad76a8c3a27124","bab60b80b15441a5ac4cb89c22d853ff","83bedcc72a7542a3bdf12abf49bace2b"]},"executionInfo":{"status":"ok","timestamp":1686416336175,"user_tz":-330,"elapsed":6249,"user":{"displayName":"","userId":""}},"outputId":"54336b8c-73e2-44ae-cb17-94bdda2b8d67","execution":{"iopub.status.busy":"2023-06-13T06:44:28.092268Z","iopub.execute_input":"2023-06-13T06:44:28.093090Z","iopub.status.idle":"2023-06-13T06:44:42.837139Z","shell.execute_reply.started":"2023-06-13T06:44:28.093059Z","shell.execute_reply":"2023-06-13T06:44:42.836052Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9207 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport nltk\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.translate.bleu_score import corpus_bleu\nnltk.download(\"punkt\")\n\n# Metric\nmetric = evaluate.load(\"bleu\")\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(pred):\n#     print(\"YO\")\n#     print(f\"pred: {pred}\")\n    # references = pred.label_ids\n    # predictions = pred.predictions.argmax(-1)\n    predictions, references = pred\n#     print(f\"references: {references}\")\n#     print(f\"predictions: {predictions}\")\n\n    # Some simple post-processing\n    # predictions, references = postprocess_text(predictions, references)\n    \n    # Convert the token indices back to strings\n    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n#     print(f\"decoded_predictions: {decoded_predictions}\")\n    references = np.where(references != -100, references, tokenizer.pad_token_id)\n    decoded_references = tokenizer.batch_decode(references, skip_special_tokens=True)\n#     print(f\"decoded_references: {decoded_references}\")\n\n    decoded_predictions, decoded_references = postprocess_text(decoded_predictions, decoded_references)\n    \n    # # Calculate BLEU score\n    # bleu = corpus_bleu([[ref] for ref in references], predictions)\n    \n    # return {\"bleu\": bleu}\n    # predictions = [tokenizer.decode(pred) for pred in predictions]\n    # references = [[tokenizer.decode(ref)] for ref in references]\n\n    \n    \n\n    # Compute BLEU score\n    bleu_score = corpus_bleu(decoded_references, decoded_predictions)\n\n    return {'bleu_score': bleu_score}","metadata":{"id":"lcYUsb5Y-kLu","colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["3cdaf71c7d73455bbf0bf1e8c29d891b","b70fa05ca74b4e40857adfe01f7e8a0a","dafae874f8c145399b0df594dba4a1f7","24252bea1ff64f62ad1799bf12cf838e","3a325d23d87e4561bd12294bebc113ab","f76c29d7b13b4398b6b4482fa541bcd8","b7f9cef2640e4722ad701f4cdb4de714","f833fe5329944d5a9801e648d77146d4","f812d8be5699420eaa21837432fbeb5f","5023e5d27caf42739d3a24f9e2b5021f","f6b0bbe20c0d4aa081669c041bc6134a"]},"executionInfo":{"status":"ok","timestamp":1686416338257,"user_tz":-330,"elapsed":2095,"user":{"displayName":"","userId":""}},"outputId":"e44c0cd1-3e4e-47bd-9d94-69823f830395","execution":{"iopub.status.busy":"2023-06-13T06:44:42.840467Z","iopub.execute_input":"2023-06-13T06:44:42.840750Z","iopub.status.idle":"2023-06-13T06:44:48.782626Z","shell.execute_reply.started":"2023-06-13T06:44:42.840724Z","shell.execute_reply":"2023-06-13T06:44:48.781733Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f878e37b78547008fc65e6f102dc262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5749f0b98e8425f99923f7971347420"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa1aadca7e24364ba2930e6d009e413"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n# we want to ignore tokenizer pad token in the loss\nlabel_pad_token_id = -100\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)","metadata":{"id":"5cnWjeXj-ma5","execution":{"iopub.status.busy":"2023-06-13T06:46:06.089136Z","iopub.execute_input":"2023-06-13T06:46:06.089856Z","iopub.status.idle":"2023-06-13T06:46:06.095308Z","shell.execute_reply.started":"2023-06-13T06:46:06.089826Z","shell.execute_reply":"2023-06-13T06:46:06.094063Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfFolder\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n\n# Hugging Face repository id\n# repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n\n# model_name = \"1_1_T_T_5e-5_2_500_D\"\n# drive_path =f\"/content/drive/MyDrive/Sprinklr_Internship/Models/{model_name}\"\nmodel_name = \"Base_5e-3_5_MB\"\ndrive_path = \"model/\"\n\n# Define training args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=drive_path,\n    per_device_train_batch_size=6,\n    per_device_eval_batch_size=12,\n    predict_with_generate=True,\n    fp16=True, # Overflows with fp16\n    learning_rate=5e-3,\n    num_train_epochs=5,\n    # logging & evaluation strategies\n    logging_dir=f\"{drive_path}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=500,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=0,\n    load_best_model_at_end=True,\n    # metric_for_best_model=\"overall_f1\",\n    # push to hub parameters\n    report_to=\"tensorboard\",\n    push_to_hub=False,\n    # hub_strategy=\"every_save\",\n    # hub_model_id=repository_id,\n    # hub_token=HfFolder.get_token(),\n)\n\n# Create Trainer instance\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"train\"],\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n    compute_metrics=compute_metrics,\n)","metadata":{"id":"EQK_PYVE-n8v","execution":{"iopub.status.busy":"2023-06-13T06:46:39.174829Z","iopub.execute_input":"2023-06-13T06:46:39.175199Z","iopub.status.idle":"2023-06-13T06:46:39.193778Z","shell.execute_reply.started":"2023-06-13T06:46:39.175151Z","shell.execute_reply":"2023-06-13T06:46:39.192731Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"id":"BL0P_Pcb-p4D","outputId":"432df492-d35c-4041-d8f3-607356a0a5d9","execution":{"iopub.status.busy":"2023-06-13T06:46:42.763583Z","iopub.execute_input":"2023-06-13T06:46:42.763970Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='486' max='7675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 486/7675 08:29 < 2:06:02, 0.95 it/s, Epoch 0.32/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"id":"tXJc8nsu-rM8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save our tokenizer and create model card\ntokenizer.save_pretrained(repository_id)\ntrainer.create_model_card()\n# Push the results to the hub\ntrainer.push_to_hub()","metadata":{"id":"LrGcTYxR-tKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nfrom random import randrange        \n\n# load model and tokenizer from huggingface hub with pipeline\nsummarizer = pipeline(\"summarization\", model=\"philschmid/flan-t5-base-samsum\", device=0)\n\n# select a random test sample\nsample = dataset['test'][randrange(len(dataset[\"test\"]))]\nprint(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n\n# summarize dialogue\nres = summarizer(sample[\"dialogue\"])\n\nprint(f\"flan-t5-base summary:\\n{res[0]['summary_text']}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8888,"status":"ok","timestamp":1686143507119,"user":{"displayName":"Tushar Bokade","userId":"07807076584166347112"},"user_tz":-330},"id":"RdYhsQxa-uiv","outputId":"9aede3fd-d5fc-4db1-e3bb-383d0d8a76a5"},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"Your max_length is set to 200, but your input_length is only 191. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n"},{"name":"stdout","output_type":"stream","text":"dialogue: \n\nNathalie: have you thought about the holiday?\n\nPauline: me & tony are into greece really\n\nJacob: anywhere warm and sunny. greece cool\n\nAnthony: greece is warm sunny and cheapish\n\nNathalie: i guess cob we ok w that\n\nJacob: sure thing\n\nPauline: so august as we said?\n\nJacob: thats the thing. we need to be back by aug 10\n\nAnthony: what?? why??\n\nNathalie: sis wedding\n\nPauline: your lil sis getting married?!? lol\n\nJacob: she's not little. seen her tony?\n\nAnthony: worth a look?\n\nNathalie: shut up assholes. shes my sister for fucks sake\n\nPauline: idiots\n\nJacob: come one just kidding. we love you\n\nAnthony: we have no choice XD\n\n---------------\n\nflan-t5-base summary:\n\nNathalie, Pauline, Anthony and Anthony are going to Greece for a holiday in August. They need to be back by August 10 because of their sister's wedding.\n"}]},{"cell_type":"markdown","source":"### GitHub Code","metadata":{"id":"EAPiimTrjgOi"}},{"cell_type":"code","source":"!git config --global user.email \"tusharbokade003@gmail.com\"\n!git config --global user.name \"Tushar Bokade\"","metadata":{"id":"mo1s73_3jkPs","executionInfo":{"status":"ok","timestamp":1686428270880,"user_tz":-330,"elapsed":504,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"token = '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'\nusername = 'suspense1441'\nrepo = 'Sprinklr'","metadata":{"id":"XCJWXc5rju6D","executionInfo":{"status":"ok","timestamp":1686428271480,"user_tz":-330,"elapsed":2,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!git clone https://{token}@github.com/{username}/{repo}","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6BQq85tjyRi","executionInfo":{"status":"ok","timestamp":1686428274105,"user_tz":-330,"elapsed":2214,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}},"outputId":"3a7e3eff-a6ca-4232-861d-5e1bd03264eb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"Cloning into 'Sprinklr'...\n\nremote: Enumerating objects: 31, done.\u001b[K\n\nremote: Counting objects: 100% (31/31), done.\u001b[K\n\nremote: Compressing objects: 100% (26/26), done.\u001b[K\n\nremote: Total 31 (delta 13), reused 13 (delta 4), pack-reused 0\u001b[K\n\nUnpacking objects: 100% (31/31), 1.93 MiB | 2.28 MiB/s, done.\n"}]},{"cell_type":"code","source":"%cd 'Sprinklr'","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9zizv_fDj0FS","executionInfo":{"status":"ok","timestamp":1686428274106,"user_tz":-330,"elapsed":4,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}},"outputId":"6c52d21f-27f3-4b50-ceb1-8de9f8bb256a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":"/content/Sprinklr\n"}]},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BRNR8spnduKw","executionInfo":{"status":"ok","timestamp":1686428133413,"user_tz":-330,"elapsed":18768,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}},"outputId":"f9cbd76d-cf3f-4a34-a1da-a9b5ae3bfe64"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Mounted at /content/drive\n"}]},{"cell_type":"code","source":"%cd ..","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFo-W7NlFiJZ","executionInfo":{"status":"ok","timestamp":1686428369801,"user_tz":-330,"elapsed":423,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}},"outputId":"1deb09cd-51dd-4148-b3cf-49b78c1fc4e5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"/content\n"}]},{"cell_type":"code","source":"!cp /content/drive/MyDrive/Sprinklr_Internship/Colab\\ Files/3.ipynb /content/Sprinklr/FLAN-T5-BASE.ipynb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0yIzJ8BwFPEW","executionInfo":{"status":"ok","timestamp":1686428408154,"user_tz":-330,"elapsed":403,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}},"outputId":"81372786-bdc7-419e-ec3f-117507ebba68"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"cp: cannot stat '/content/drive/MyDrive/Sprinklr_Internship/Colab Files/3.ipynb': No such file or directory\n"}]},{"cell_type":"code","source":"!git status","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nn-I2FPDj1-u","executionInfo":{"status":"ok","timestamp":1686318917540,"user_tz":-330,"elapsed":517,"user":{"displayName":"","userId":""}},"outputId":"d419712a-9169-4c66-a2a1-3080829e9c36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"On branch main\n\nYour branch is up to date with 'origin/main'.\n\n\n\nnothing to commit, working tree clean\n"}]},{"cell_type":"code","source":"!git add --all","metadata":{"id":"wMKfEkHkj5b8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git commit -a -m \"Rouge\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-u_SNGI5j7LT","executionInfo":{"status":"ok","timestamp":1686312724612,"user_tz":-330,"elapsed":608,"user":{"displayName":"","userId":""}},"outputId":"ef050f09-3832-4a15-c6d2-2a82a5495048"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"On branch main\n\nYour branch is up to date with 'origin/main'.\n\n\n\nnothing to commit, working tree clean\n"}]},{"cell_type":"code","source":"!git remote -v","metadata":{"id":"zmkplsT0j-jX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git push origin main","metadata":{"id":"tQJKJ90ekAFw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import locale\nlocale.getpreferredencoding = lambda: \"UTF-8\"","metadata":{"id":"dT1Ib_7kcGCC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv /content/drive/MyDrive/Colab\\ Notebooks/3.ipynb /content/drive/MyDrive/Sprinklr_Internship/Colab\\ Files","metadata":{"id":"C97CwiCrcGSd","executionInfo":{"status":"ok","timestamp":1686428235525,"user_tz":-330,"elapsed":774,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"function ClickConnect(){\nconsole.log(\"Working\"); \ndocument.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n}\ntimerId = setInterval(ClickConnect,1000000000)\nclearInterval(timerId);","metadata":{"id":"JjqZEcC0d8Mw"},"execution_count":null,"outputs":[]}]}