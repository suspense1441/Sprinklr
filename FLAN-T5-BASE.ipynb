{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"s53pqSYe9txe"},"outputs":[],"source":["%%capture\n","!pip install pytesseract transformers datasets rouge-score nltk tensorboard py7zr accelerate evaluate --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_7iFA66Q95dY"},"outputs":[],"source":["%%capture\n","!sudo apt-get install git-lfs --yes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"badcwQ6g-ccH"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","model_id=\"google/flan-t5-base\"\n","\n","# load model from the hub\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n","\n","# Load tokenizer of FLAN-t5-base\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gm2JKgq2syEc"},"outputs":[],"source":["from datasets import load_dataset\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3oUPk5JDPEp"},"outputs":[],"source":["dataset = load_dataset(\"json\", data_files=\"./dollyFinal.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqBxblpfDu_X"},"outputs":[],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sa4eEgsRDMhv"},"outputs":[],"source":["# train_indices, test_indices = train_test_split(range(len(dataset['train'])), test_size=0.1, random_state=42)\n","\n","# train_ds = dataset['train'].select(train_indices).map(lambda example: {k: example[k] for k in desired_columns})\n","# test_ds = dataset['train'].select(test_indices).map(lambda example: {k: example[k] for k in desired_columns})\n","\n","# dataset['train'] = train_ds\n","# dataset['test'] = test_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1LVQgB2-eX-"},"outputs":[],"source":["# from datasets import concatenate_datasets\n","\n","# The maximum total input sequence length after tokenization. \n","# Sequences longer than this will be truncated, sequences shorter will be padded.\n","tokenized_inputs = dataset[\"train\"].map(lambda x: tokenizer(x[\"data\"], truncation=True), batched=True, remove_columns=[\"data\", \"response\"])\n","max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n","print(f\"Max source length: {max_source_length}\")\n","\n","# The maximum total sequence length for target text after tokenization. \n","# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n","tokenized_targets = dataset[\"train\"].map(lambda x: tokenizer(x[\"response\"], truncation=True), batched=True, remove_columns=[\"data\", \"response\"])\n","max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n","print(f\"Max target length: {max_target_length}\")"]},{"cell_type":"code","source":["dataset['train'][0]"],"metadata":{"id":"YWTIijYBtvFG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55MVJ0f--gRj"},"outputs":[],"source":["def preprocess_function(sample,padding=\"max_length\"):\n","    # # add prefix to the input for t5\n","    inputs = sample[\"data\"]\n","\n","    # # tokenize inputs\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n","\n","    # Tokenize targets with the `text_target` keyword argument\n","    labels = tokenizer(text_target=sample[\"response\"], max_length=max_target_length, padding=padding, truncation=True)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"data\", \"response\"])\n","print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcYUsb5Y-kLu"},"outputs":[],"source":["import evaluate\n","import nltk\n","import numpy as np\n","from nltk.tokenize import sent_tokenize\n","nltk.download(\"punkt\")\n","\n","# Metric\n","metric = evaluate.load(\"rouge\")\n","\n","# helper function to postprocess text\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    # rougeLSum expects newline after each sentence\n","    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n","\n","    return preds, labels\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {k: round(v * 100, 4) for k, v in result.items()}\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cnWjeXj-ma5"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","# we want to ignore tokenizer pad token in the loss\n","label_pad_token_id = -100\n","# Data collator\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer,\n","    model=model,\n","    label_pad_token_id=label_pad_token_id,\n","    pad_to_multiple_of=8\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQK_PYVE-n8v"},"outputs":[],"source":["from huggingface_hub import HfFolder\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","# Hugging Face repository id\n","# repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n","\n","model_name = \"8_8_False_5e-5_5_500\"\n","drive_path =\"/content/drive/MyDrive/Sprinklr_Internship/Models/{model_name}\"\n","\n","# Define training args\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=drive_path,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=1,\n","    predict_with_generate=True,\n","    fp16=True, # Overflows with fp16\n","    learning_rate=5e-5,\n","    num_train_epochs=2,\n","    # logging & evaluation strategies\n","    logging_dir=f\"{drive_path}/logs\",\n","    logging_strategy=\"steps\",\n","    logging_steps=500,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=2,\n","    load_best_model_at_end=True,\n","    # metric_for_best_model=\"overall_f1\",\n","    # push to hub parameters\n","    report_to=\"tensorboard\",\n","    push_to_hub=False,\n","    # hub_strategy=\"every_save\",\n","    # hub_model_id=repository_id,\n","    # hub_token=HfFolder.get_token(),\n",")\n","\n","# Create Trainer instance\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"train\"],\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"BL0P_Pcb-p4D","outputId":"731924ba-e3ec-4a79-bbc1-810bee6e68e7","executionInfo":{"status":"error","timestamp":1686311817045,"user_tz":-330,"elapsed":11,"user":{"displayName":"","userId":""}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-635967aca760>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"]}],"source":["# Start training\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXJc8nsu-rM8"},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrGcTYxR-tKg"},"outputs":[],"source":["# Save our tokenizer and create model card\n","tokenizer.save_pretrained(repository_id)\n","trainer.create_model_card()\n","# Push the results to the hub\n","trainer.push_to_hub()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8888,"status":"ok","timestamp":1686143507119,"user":{"displayName":"Tushar Bokade","userId":"07807076584166347112"},"user_tz":-330},"id":"RdYhsQxa-uiv","outputId":"9aede3fd-d5fc-4db1-e3bb-383d0d8a76a5"},"outputs":[{"name":"stderr","output_type":"stream","text":["Your max_length is set to 200, but your input_length is only 191. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n"]},{"name":"stdout","output_type":"stream","text":["dialogue: \n","Nathalie: have you thought about the holiday?\r\n","Pauline: me & tony are into greece really\r\n","Jacob: anywhere warm and sunny. greece cool\r\n","Anthony: greece is warm sunny and cheapish\r\n","Nathalie: i guess cob we ok w that\r\n","Jacob: sure thing\r\n","Pauline: so august as we said?\r\n","Jacob: thats the thing. we need to be back by aug 10\r\n","Anthony: what?? why??\r\n","Nathalie: sis wedding\r\n","Pauline: your lil sis getting married?!? lol\r\n","Jacob: she's not little. seen her tony?\r\n","Anthony: worth a look?\r\n","Nathalie: shut up assholes. shes my sister for fucks sake\r\n","Pauline: idiots\r\n","Jacob: come one just kidding. we love you\r\n","Anthony: we have no choice XD\n","---------------\n","flan-t5-base summary:\n","Nathalie, Pauline, Anthony and Anthony are going to Greece for a holiday in August. They need to be back by August 10 because of their sister's wedding.\n"]}],"source":["from transformers import pipeline\n","from random import randrange        \n","\n","# load model and tokenizer from huggingface hub with pipeline\n","summarizer = pipeline(\"summarization\", model=\"philschmid/flan-t5-base-samsum\", device=0)\n","\n","# select a random test sample\n","sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n","print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n","\n","# summarize dialogue\n","res = summarizer(sample[\"dialogue\"])\n","\n","print(f\"flan-t5-base summary:\\n{res[0]['summary_text']}\")"]},{"cell_type":"markdown","source":["### GitHub Code"],"metadata":{"id":"EAPiimTrjgOi"}},{"cell_type":"code","source":["!git config --global user.email \"tusharbokade003@gmail.com\"\n","!git config --global user.name \"Tushar Bokade\""],"metadata":{"id":"mo1s73_3jkPs","executionInfo":{"status":"ok","timestamp":1686312880474,"user_tz":-330,"elapsed":768,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["token = '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'\n","username = 'suspense1441'\n","repo = 'Sprinklr'"],"metadata":{"id":"XCJWXc5rju6D","executionInfo":{"status":"ok","timestamp":1686312881131,"user_tz":-330,"elapsed":659,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!git clone https://{token}@github.com/{username}/{repo}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6BQq85tjyRi","executionInfo":{"status":"ok","timestamp":1686312883105,"user_tz":-330,"elapsed":1978,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}},"outputId":"4b26d6c7-f014-4773-d62c-f014d5fdaa81"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Sprinklr'...\n","remote: Enumerating objects: 25, done.\u001b[K\n","remote: Counting objects: 100% (25/25), done.\u001b[K\n","remote: Compressing objects: 100% (21/21), done.\u001b[K\n","remote: Total 25 (delta 10), reused 8 (delta 3), pack-reused 0\u001b[K\n","Unpacking objects: 100% (25/25), 1.91 MiB | 1.99 MiB/s, done.\n"]}]},{"cell_type":"code","source":["%cd 'Sprinklr'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9zizv_fDj0FS","executionInfo":{"status":"ok","timestamp":1686312883106,"user_tz":-330,"elapsed":4,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}},"outputId":"9ded2e13-b42f-4f23-d667-0dfe1c151137"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Sprinklr\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k-4dpiRWcZYM","executionInfo":{"status":"ok","timestamp":1686313017376,"user_tz":-330,"elapsed":429,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}},"outputId":"c383e604-a884-48ab-ab32-a1a7e8a1e6c1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data  Sprinklr\n"]}]},{"cell_type":"code","source":["%cd .."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xdyPbDqTceZc","executionInfo":{"status":"ok","timestamp":1686302248004,"user_tz":-330,"elapsed":2,"user":{"displayName":"","userId":""}},"outputId":"f717aed2-7b6c-4b1d-9a8a-f9542339ba59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BRNR8spnduKw","executionInfo":{"status":"ok","timestamp":1686312908370,"user_tz":-330,"elapsed":18643,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}},"outputId":"b0a70970-ad67-4069-9aa4-8dc5e549cf60"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd .."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFo-W7NlFiJZ","executionInfo":{"status":"ok","timestamp":1686312991102,"user_tz":-330,"elapsed":2,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}},"outputId":"4c90e642-04cb-41fc-deb1-8f5aa972b679"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!mv /content/drive/MyDrive/Colab /content/Sprinklr/FLAN-T5-BASE.ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0yIzJ8BwFPEW","executionInfo":{"status":"ok","timestamp":1686313026445,"user_tz":-330,"elapsed":4,"user":{"displayName":"TUSHAR BOKADE","userId":"16108262612284869135"}},"outputId":"1514a0b6-459d-48c1-94f1-bee635244e8e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["mv: cannot stat '/drive/MyDrive/Colab': No such file or directory\n"]}]},{"cell_type":"code","source":["!git status"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nn-I2FPDj1-u","executionInfo":{"status":"ok","timestamp":1686312633911,"user_tz":-330,"elapsed":4,"user":{"displayName":"","userId":""}},"outputId":"1489a719-0f9b-4094-8c7d-2c56ef91a1cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","nothing to commit, working tree clean\n"]}]},{"cell_type":"code","source":["!git add --all"],"metadata":{"id":"wMKfEkHkj5b8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -a -m \"Rouge\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-u_SNGI5j7LT","executionInfo":{"status":"ok","timestamp":1686312724612,"user_tz":-330,"elapsed":608,"user":{"displayName":"","userId":""}},"outputId":"ef050f09-3832-4a15-c6d2-2a82a5495048"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","nothing to commit, working tree clean\n"]}]},{"cell_type":"code","source":["!git remote -v"],"metadata":{"id":"zmkplsT0j-jX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git push origin main"],"metadata":{"id":"tQJKJ90ekAFw"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/suspense1441/Sprinklr/blob/main/FLAN-T5-BASE.ipynb","timestamp":1686312776497}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}